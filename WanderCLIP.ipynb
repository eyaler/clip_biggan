{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WanderCLIP.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMwr3WmauPPDKWV4q3oef82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyaler/clip_biggan/blob/main/WanderCLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwTP4MYk0bYn"
      },
      "source": [
        "# BigGAN + CLIP + CMA-ES\r\n",
        "\r\n",
        "[j.mp/bigclip](https://j.mp/bigclip)\r\n",
        "\r\n",
        "By Eyal Gruss [@eyaler](https://twitter.com/eyaler) [eyalgruss.com](https://eyalgruss.com)\r\n",
        "\r\n",
        "Based on SIREN+CLIP Colabs by: [@advadnoun](https://twitter.com/advadnoun), [@norod78](https://twitter.com/norod78)\r\n",
        "\r\n",
        "Other CLIP notebooks: [OpenAI tutorial](https://colab.research.google.com/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb), [SIREN by @advadnoun](https://colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP), [SIREN by @norod78](https://colab.research.google.com/drive/1K1vfpTEvAmxW2rnhAaALRVyis8EiLOnD), [BigGAN by @advadnoun](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR), [BigGAN by @eyaler](j.mp/bigclip), [BigGAN by @tg_bomze](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/Text2Image_v2.ipynb), [BigGAN using big-sleep library by @lucidrains](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb), [BigGAN story hallucinator by @bonkerfield](https://colab.research.google.com/drive/1jF8pyZ7uaNYbk9ZiVdxTOajkp8kbmkLK), [StyleGAN2-ADA Anime by @nagolinc](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/TADNE_and_CLIP.ipynb)\r\n",
        "\r\n",
        "Using the works:\r\n",
        "\r\n",
        "https://github.com/openai/CLIP\r\n",
        "\r\n",
        "https://tfhub.dev/deepmind/biggan-deep-512\r\n",
        "\r\n",
        "https://github.com/huggingface/pytorch-pretrained-BigGAN\r\n",
        "\r\n",
        "http://www.aiartonline.com/design-2019/eyal-gruss (WanderGAN)\r\n",
        "\r\n",
        "For a curated list of more online generative tools see: [j.mp/generativetools](https://j.mp/generativetools)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWmKTmvBg7z5",
        "cellView": "form"
      },
      "source": [
        "#@title Restart after running this cell!\r\n",
        "\r\n",
        "!nvidia-smi -L\r\n",
        "\r\n",
        "import subprocess\r\n",
        "\r\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\r\n",
        "print(\"CUDA version:\", CUDA_version)\r\n",
        "\r\n",
        "if CUDA_version == \"10.0\":\r\n",
        "    torch_version_suffix = \"+cu100\"\r\n",
        "elif CUDA_version == \"10.1\":\r\n",
        "    torch_version_suffix = \"+cu101\"\r\n",
        "elif CUDA_version == \"10.2\":\r\n",
        "    torch_version_suffix = \"\"\r\n",
        "else:\r\n",
        "    torch_version_suffix = \"+cu110\"\r\n",
        "\r\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SDpkkK7cU1y",
        "cellView": "form"
      },
      "source": [
        "#@title Setup\r\n",
        "!pip install pytorch-pretrained-biggan\r\n",
        "from pytorch_pretrained_biggan import BigGAN\r\n",
        "gan_model = BigGAN.from_pretrained('biggan-deep-512').cuda().eval()\r\n",
        "\r\n",
        "%cd /content\r\n",
        "!git clone --depth 1 https://github.com/openai/CLIP\r\n",
        "!pip install ftfy\r\n",
        "%cd /content/CLIP\r\n",
        "import clip\r\n",
        "models = clip.available_models()\r\n",
        "perceptor={}\r\n",
        "preprocess={}\r\n",
        "for model in models:\r\n",
        "  perceptor[model], preprocess[model] = clip.load(model)\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "!pip install cma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOWzPLrBbdxW",
        "cellView": "form"
      },
      "source": [
        "#@title Generate!\r\n",
        "#@markdown 1. For **prompt** OpenAI suggest to use the template \"A photo of a X.\" or \"A photo of a X, a type of Y.\" [[paper]](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)\r\n",
        "#@markdown 2. For **initial_class** you can either use free text or select a special option from the drop-down list.\r\n",
        "#@markdown 3. Free text and 'From prompt' might fail to find an appropriate ImageNet class.\r\n",
        "#@markdown 4. **seed**=0 means no seed.\r\n",
        "prompt = 'A photo of a rainbow unicorn.' #@param {type:'string'}\r\n",
        "initial_class = 'Random mix' #@param ['From prompt', 'Random class', 'Random Dirichlet', 'Random mix'] {allow-input: true}\r\n",
        "optimize_class = True #@param {type:'boolean'}\r\n",
        "class_smoothing = 0.1 #@param {type:'number'}\r\n",
        "truncation = 1 #@param {type:'number'}\r\n",
        "stochastic_truncation = False #@param {type:'boolean'}\r\n",
        "optimizer = 'CMA-ES' #@param ['SGD','Adam','CMA-ES','CMA-ES+SGD','CMA-ES+Adam']\r\n",
        "pop_size = 50 #@param {type:'integer'}\r\n",
        "model = 'ViT-B/32' #@param ['ViT-B/32','RN50']\r\n",
        "augmentations =  64#@param {type:'integer'}\r\n",
        "learning_rate = 0.1 #@param {type:'number'}\r\n",
        "class_ent_reg = 0.0001 #@param {type:'number'}\r\n",
        "iterations = 100 #@param {type:'integer'}\r\n",
        "save_every = 1 #@param {type:'integer'}\r\n",
        "fps = 1 #@param {type:'number'}\r\n",
        "freeze_secs = 0 #@param {type:'number'}\r\n",
        "seed = 0 #@param {type:'number'}\r\n",
        "if seed == 0:\r\n",
        "  seed = None\r\n",
        "if 'CMA' not in optimizer:\r\n",
        "  pop_size = 1\r\n",
        "\r\n",
        "!rm -rf /content/output\r\n",
        "!mkdir -p /content/output\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "state = None if not seed else np.random.RandomState(seed)\r\n",
        "np.random.seed(seed)\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import sys\r\n",
        "torch.manual_seed(np.random.randint(sys.maxsize))\r\n",
        "import imageio\r\n",
        "from IPython.display import HTML, Image, clear_output\r\n",
        "from scipy.stats import truncnorm, dirichlet\r\n",
        "from pytorch_pretrained_biggan import convert_to_images, one_hot_from_names\r\n",
        "from base64 import b64encode\r\n",
        "from time import time\r\n",
        "import cma\r\n",
        "from cma.sigma_adaptation import CMAAdaptSigmaCSA, CMAAdaptSigmaTPA\r\n",
        "\r\n",
        "im_shape = [512, 512, 3]\r\n",
        "sideX, sideY, channels = im_shape\r\n",
        "\r\n",
        "def save(out,name):\r\n",
        "  with torch.no_grad():\r\n",
        "    out = out.cpu().numpy()\r\n",
        "  img = convert_to_images(out)[0]\r\n",
        "  imageio.imwrite(name, np.asarray(img))\r\n",
        "\r\n",
        "def checkin(i, best_ind, total_losses, losses, regs, values, out):\r\n",
        "  global sample_num\r\n",
        "  name = '/content/output/frame_%05d.jpg'%sample_num\r\n",
        "  save(out,name)\r\n",
        "  clear_output()\r\n",
        "  display(Image(name))  \r\n",
        "  best = values[best_ind]\r\n",
        "  inds = np.argsort(best)[::-1]\r\n",
        "  values = np.array(values)\r\n",
        "  print('sample=%d iter=%d best: total=%.2f cos=%.2f reg=%.3f avg: total=%.2f cos=%.2f reg=%.3f std: total=%.2f cos=%.2f reg=%.3f 1st_class=%s (%.2f) 2nd_class=%s (%.2f) 3rd_class=%s (%.2f) components: >=0.5=%.0f, >=0.3=%.0f, >=0.1=%.0f'%(sample_num+1, i+1, total_losses[best_ind], losses[best_ind], regs[best_ind], np.mean(total_losses), np.mean(losses), np.mean(regs), np.std(total_losses), np.std(losses), np.std(regs), inds[0], best[inds[0]], inds[1], best[inds[1]], inds[2], best[inds[2]], np.sum(values >= 0.5)/pop_size,np.sum(values >= 0.3)/pop_size,np.sum(values >= 0.1)/pop_size))\r\n",
        "  sample_num += 1\r\n",
        "\r\n",
        "noise_vector = truncnorm.rvs(-2*truncation, 2*truncation, size=(pop_size, 128), random_state=state).astype(np.float32) #see https://github.com/tensorflow/hub/issues/214\r\n",
        "\r\n",
        "if initial_class.lower()=='random class':\r\n",
        "  class_vector = np.ones(shape=(pop_size,1000), dtype=np.float32)*class_smoothing/999\r\n",
        "  class_vector[0,np.random.randint(1000)] = 1-class_smoothing\r\n",
        "elif initial_class.lower()=='random dirichlet':\r\n",
        "  class_vector = dirichlet.rvs([pop_size/1000] * 1000, size=1, random_state=state).astype(np.float32)\r\n",
        "elif initial_class.lower()=='random mix':\r\n",
        "  class_vector = np.random.rand(pop_size,1000).astype(np.float32)\r\n",
        "else:\r\n",
        "  if initial_class.lower()=='from prompt':\r\n",
        "    initial_class = prompt\r\n",
        "  try:\r\n",
        "    class_vector = None\r\n",
        "    class_vector = one_hot_from_names(initial_class, batch_size=pop_size)\r\n",
        "    assert class_vector is not None\r\n",
        "    class_vector = class_vector*(1-class_smoothing*1000/999)+class_smoothing/999\r\n",
        "  except Exception as e:  \r\n",
        "    print('Error: could not find initial_class. Try something else.')\r\n",
        "    raise e\r\n",
        "\r\n",
        "eps = 1e-8\r\n",
        "class_vector = np.log(class_vector+eps)\r\n",
        "noise_vector = torch.tensor(noise_vector, requires_grad='SGD' in optimizer or 'Adam' in optimizer, device='cuda')\r\n",
        "class_vector = torch.tensor(class_vector, requires_grad='SGD' in optimizer or 'Adam' in optimizer, device='cuda')\r\n",
        "\r\n",
        "if 'SGD' in optimizer or 'Adam' in optimizer:\r\n",
        "  params = [noise_vector]\r\n",
        "  if optimize_class:\r\n",
        "    params = params + [class_vector]\r\n",
        "  if 'SGD' in optimizer:\r\n",
        "    optim = torch.optim.SGD(params, lr=learning_rate)  \r\n",
        "  else:\r\n",
        "    optim = torch.optim.Adam(params, lr=learning_rate)\r\n",
        "\r\n",
        "tx = clip.tokenize(prompt)\r\n",
        "with torch.no_grad():\r\n",
        "  target_clip = perceptor[model].encode_text(tx.cuda())\r\n",
        "\r\n",
        "def get_output(noise_vector, class_vector):\r\n",
        "  if stochastic_truncation:\r\n",
        "    with torch.no_grad():\r\n",
        "      trunc_indices = torch.abs(noise_vector) > 2*truncation\r\n",
        "      size = torch.count_nonzero(trunc_indices).cpu().numpy()\r\n",
        "      trunc = truncnorm.rvs(-2*truncation, 2*truncation, size=(1,size)).astype(np.float32)\r\n",
        "      noise_vector.data[trunc_indices] = torch.tensor(trunc, requires_grad='SGD' in optimizer or 'Adam' in optimizer, device='cuda')\r\n",
        "  else:\r\n",
        "    noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\r\n",
        "  class_vector_norm = class_vector.softmax(dim=-1)\r\n",
        "  return gan_model(noise_vector, class_vector_norm, truncation), class_vector_norm\r\n",
        "\r\n",
        "res = perceptor[model].input_resolution.item()\r\n",
        "smoothed_ent = -torch.tensor(class_smoothing*np.log(class_smoothing/999+eps)+(1-class_smoothing)*np.log(1-class_smoothing+eps), dtype=torch.float32).cuda()\r\n",
        "def ascend_txt(i, grad_step=False, show_save=False):\r\n",
        "  prev_class_vector_norms = []\r\n",
        "  regs = []\r\n",
        "  losses = []\r\n",
        "  total_losses = []\r\n",
        "  best_loss = np.inf\r\n",
        "  for j in range(pop_size):\r\n",
        "    out, class_vector_norm = get_output(noise_vector[j:j+1], class_vector[j:j+1])\r\n",
        "    with torch.no_grad():\r\n",
        "      prev_class_vector_norms.append(class_vector_norm.cpu().numpy()[0])\r\n",
        "    p_s = []\r\n",
        "    fixed_out = (out+1)/2\r\n",
        "    for ch in range(augmentations):\r\n",
        "      size = torch.randint(int(.5*sideX), int(.98*sideX), ())\r\n",
        "      #size = int(sideX*torch.zeros(1,).normal_(mean=.8, std=.3).clip(.5, .95))\r\n",
        "      offsetx = torch.randint(0, sideX - size, ())\r\n",
        "      offsety = torch.randint(0, sideX - size, ())\r\n",
        "      apper = fixed_out[:, :, offsetx:offsetx + size, offsety:offsety + size]\r\n",
        "      apper = torch.nn.functional.interpolate(apper, res, mode='bicubic')\r\n",
        "      apper = apper.clamp(0,1)\r\n",
        "      p_s.append(apper)\r\n",
        "    into = nom(torch.cat(p_s, 0))\r\n",
        "    predict_clip = perceptor[model].encode_image(into)\r\n",
        "    factor = 100\r\n",
        "    loss = factor*(1-torch.cosine_similarity(predict_clip, target_clip).mean())\r\n",
        "    total_loss = loss\r\n",
        "    if optimize_class and class_ent_reg:\r\n",
        "      reg = factor*class_ent_reg*((-class_vector_norm*torch.log(class_vector_norm+eps)).sum()-smoothed_ent).abs()\r\n",
        "      total_loss = total_loss + reg\r\n",
        "      with torch.no_grad():\r\n",
        "        regs.append(reg.item())\r\n",
        "    with torch.no_grad():\r\n",
        "      losses.append(loss.item())\r\n",
        "      total_losses.append(total_loss.item())\r\n",
        "    if total_losses[-1]<best_loss:\r\n",
        "      best_loss = total_losses[-1]\r\n",
        "      best_ind = j\r\n",
        "      best_out = out\r\n",
        "    if grad_step:    \r\n",
        "      optim.zero_grad()\r\n",
        "      total_loss.backward()\r\n",
        "      optim.step()\r\n",
        "      \r\n",
        "  if show_save and (i == iterations-1 or i % save_every == 0):\r\n",
        "    if i==iterations-1:\r\n",
        "      save(best_out,'/content/%s.jpg'%prompt)  \r\n",
        "    if i % save_every == 0:\r\n",
        "      checkin(i, best_ind, total_losses, losses, regs, prev_class_vector_norms, best_out)  \r\n",
        "  return total_losses\r\n",
        "\r\n",
        "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\r\n",
        "if 'CMA' in optimizer:\r\n",
        "  cma_opts = {'popsize': pop_size, 'seed': np.nan, 'AdaptSigma': True, 'CMA_diagonal': True}\r\n",
        "  cmaes = cma.CMAEvolutionStrategy([0]*(1128 if optimize_class else 128), 1, inopts=cma_opts)\r\n",
        "\r\n",
        "sample_num = 0\r\n",
        "machine = !nvidia-smi -L\r\n",
        "start = time()\r\n",
        "for i in range(iterations):    \r\n",
        "  if 'CMA' in optimizer:\r\n",
        "    with torch.no_grad():\r\n",
        "      cma_results = torch.tensor(cmaes.ask(), dtype=torch.float32).cuda()\r\n",
        "      if optimize_class:\r\n",
        "        noise_vector.data, class_vector.data = torch.split_with_sizes(cma_results, (128,1000), dim=-1)\r\n",
        "      else:\r\n",
        "        noise_vector.data = cma_results\r\n",
        "      losses = ascend_txt(i, show_save='SGD' not in optimizer and 'Adam' not in optimizer)\r\n",
        "  if 'SGD' in optimizer or 'Adam' in optimizer:\r\n",
        "    losses = ascend_txt(i, grad_step=True, show_save=True)\r\n",
        "    assert noise_vector.requires_grad and noise_vector.is_leaf and (class_vector.requires_grad and class_vector.is_leaf or not optimize_class), (noise_vector.requires_grad, noise_vector.is_leaf, class_vector.requires_grad, class_vector.is_leaf)\r\n",
        "  if 'CMA' in optimizer:\r\n",
        "    with torch.no_grad():\r\n",
        "      if optimize_class:\r\n",
        "        vectors = torch.cat([noise_vector,class_vector], dim=1)\r\n",
        "      else:\r\n",
        "        vectors = noise_vector\r\n",
        "      cmaes.tell(vectors.cpu().numpy(), losses)\r\n",
        "  print('took: %d secs (%.2f sec/iter) on %s'%(time()-start,(time()-start)/(i+1), machine[0]))\r\n",
        "\r\n",
        "from google.colab import files, output\r\n",
        "files.download('/content/%s.jpg'%prompt)\r\n",
        "\r\n",
        "out = '\"/content/%s.mp4\"'%prompt\r\n",
        "with open('/content/list.txt','w') as f:\r\n",
        "  for i in range(iterations):\r\n",
        "    f.write('file /content/output/frame_%05d.jpg\\n'%i)\r\n",
        "  for j in range(int(freeze_secs*fps)):\r\n",
        "    f.write('file /content/output/frame_%05d.jpg\\n'%i)\r\n",
        "!ffmpeg -r $fps -f concat -safe 0 -i /content/list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\r\n",
        "with open('/content/%s.mp4'%prompt, 'rb') as f:\r\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(f.read()).decode()\r\n",
        "display(HTML(\"\"\"\r\n",
        "  <video controls autoplay loop>\r\n",
        "        <source src=\"%s\" type=\"video/mp4\">\r\n",
        "  </video>\"\"\" % data_url))\r\n",
        "\r\n",
        "from google.colab import files, output\r\n",
        "output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\r\n",
        "files.download('/content/%s.mp4'%prompt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}