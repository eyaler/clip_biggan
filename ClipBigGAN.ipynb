{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClipBigGAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPYKEc9j7GOcKEeAnGYhQ58",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyaler/clip_biggan/blob/main/ClipBigGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwTP4MYk0bYn"
      },
      "source": [
        "# BigGAN + CLIP\r\n",
        "\r\n",
        "by https://twitter.com/eyaler\r\n",
        "\r\n",
        "based on colabs by:\r\n",
        "\r\n",
        "https://twitter.com/advadnoun\r\n",
        "\r\n",
        "https://twitter.com/norod78\r\n",
        "\r\n",
        "based on the works:\r\n",
        "\r\n",
        "https://github.com/openai/CLIP\r\n",
        "\r\n",
        "https://tfhub.dev/deepmind/biggan-deep-512\r\n",
        "\r\n",
        "https://github.com/huggingface/pytorch-pretrained-BigGAN\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EWmKTmvBg7z5"
      },
      "source": [
        "#@title Restart after running this cell!\r\n",
        "\r\n",
        "import subprocess\r\n",
        "\r\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\r\n",
        "print(\"CUDA version:\", CUDA_version)\r\n",
        "\r\n",
        "if CUDA_version == \"10.0\":\r\n",
        "    torch_version_suffix = \"+cu100\"\r\n",
        "elif CUDA_version == \"10.1\":\r\n",
        "    torch_version_suffix = \"+cu101\"\r\n",
        "elif CUDA_version == \"10.2\":\r\n",
        "    torch_version_suffix = \"\"\r\n",
        "else:\r\n",
        "    torch_version_suffix = \"+cu110\"\r\n",
        "\r\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1SDpkkK7cU1y"
      },
      "source": [
        "#@title Setup\r\n",
        "!pip install pytorch-pretrained-biggan\r\n",
        "from pytorch_pretrained_biggan import BigGAN\r\n",
        "model = BigGAN.from_pretrained('biggan-deep-512').cuda()\r\n",
        "\r\n",
        "%cd /content\r\n",
        "!git clone --depth 1 https://github.com/openai/CLIP\r\n",
        "!pip install ftfy\r\n",
        "%cd /content/CLIP\r\n",
        "import clip\r\n",
        "perceptor, preprocess = clip.load('ViT-B/32')\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('wordnet')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AOWzPLrBbdxW"
      },
      "source": [
        "#@title Optimize!\r\n",
        "#@markdown note for initial_class you can also enter free text\r\n",
        "prompt = 'Green mushroom' #@param {type:'string'}\r\n",
        "initial_class = 'mushroom' #@param ['from prompt', 'random class', 'random dirichlet', 'random mix'] {allow-input: true}\r\n",
        "optimize_class = True #@param {type:'boolean'}\r\n",
        "truncation = 1 #@param {type:'number'}\r\n",
        "learning_rate =  0.1#@param {type:'number'}\r\n",
        "class_ent_reg =  0.1#@param {type:'number'}\r\n",
        "iterations = 500 #@param {type:'integer'}\r\n",
        "save_every = 1 #@param {type:'integer'}\r\n",
        "fps = 30 #@param {type:'integer'}\r\n",
        "\r\n",
        "!rm -rf /content/output\r\n",
        "!mkdir -p /content/output\r\n",
        "\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import numpy as np\r\n",
        "import imageio\r\n",
        "from IPython.display import HTML, Image, clear_output\r\n",
        "from scipy.stats import truncnorm, dirichlet\r\n",
        "from pytorch_pretrained_biggan import convert_to_images, one_hot_from_names\r\n",
        "from base64 import b64encode\r\n",
        "\r\n",
        "im_shape = [512, 512, 3]\r\n",
        "sideX, sideY, channels = im_shape\r\n",
        "\r\n",
        "def save(out,name):\r\n",
        "  with torch.no_grad():\r\n",
        "    al = out.cpu().numpy()\r\n",
        "  img = convert_to_images(al)[0]\r\n",
        "  imageio.imwrite(name, np.asarray(img))\r\n",
        "\r\n",
        "def checkin(total_loss, loss, reg, values, out):\r\n",
        "  global sample_num\r\n",
        "  name = '/content/output/frame_%05d.jpg'%sample_num\r\n",
        "  save(out,name)\r\n",
        "  clear_output()\r\n",
        "  display(Image(name))\r\n",
        "  print('%d: total=%.1f cos=%.1f reg=%.1f components: >=0.5=%d, >=0.3=%d, >=0.1=%d\\n'%(sample_num, total_loss, loss, reg,np.sum(values >= 0.5),np.sum(values >= 0.3),np.sum(values >= 0.1)))\r\n",
        "  \r\n",
        "  sample_num +=1\r\n",
        "\r\n",
        "seed = None\r\n",
        "state = None if seed is None else np.random.RandomState(seed)\r\n",
        "np.random.seed(seed)\r\n",
        "noise_vector = truncnorm.rvs(-2*truncation, 2*truncation, size=(1, 128), random_state=state).astype(np.float32) #see https://github.com/tensorflow/hub/issues/214\r\n",
        "\r\n",
        "if initial_class=='random class':\r\n",
        "  class_vector = np.zeros(shape=(1,1000), dtype=np.float32)\r\n",
        "  class_vector[0,np.random.randint(1000)] = 1\r\n",
        "elif initial_class=='random dirichlet':\r\n",
        "  class_vector = dirichlet.rvs([1/1000] * 1000, size=1, random_state=state).astype(np.float32)\r\n",
        "elif initial_class=='random mix':\r\n",
        "  class_vector = np.random.rand(1,1000).astype(np.float32)\r\n",
        "else:\r\n",
        "  if initial_class=='from prompt':\r\n",
        "    initial_class = prompt\r\n",
        "  try:\r\n",
        "    class_vector = one_hot_from_names(initial_class, batch_size=1)\r\n",
        "    assert class_vector is not None\r\n",
        "  except Exception:  \r\n",
        "    print('Error: could not find initial_class. Try something else.')\r\n",
        "eps=1e-8\r\n",
        "class_vector = np.log(class_vector+eps)\r\n",
        "\r\n",
        "# All in tensors\r\n",
        "noise_vector = torch.tensor(noise_vector, requires_grad=True, device='cuda')\r\n",
        "class_vector = torch.tensor(class_vector, requires_grad=True, device='cuda')\r\n",
        "\r\n",
        "params = [noise_vector]\r\n",
        "if optimize_class:\r\n",
        "  params += [class_vector]\r\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\r\n",
        "\r\n",
        "tx = clip.tokenize(prompt)\r\n",
        "with torch.no_grad():\r\n",
        "  target_clip = perceptor.encode_text(tx.cuda())\r\n",
        "\r\n",
        "def ascend_txt(i):\r\n",
        "  noise_vector_trunc = noise_vector.clamp(-2*truncation,2*truncation)\r\n",
        "  class_vector_norm = torch.nn.functional.softmax(class_vector)\r\n",
        "  out = model(noise_vector_trunc, class_vector_norm, truncation)\r\n",
        "  if i==iterations-1:\r\n",
        "    save(out,'/content/%s.jpg'%prompt)\r\n",
        "  cutn = 64\r\n",
        "  p_s = []\r\n",
        "  for ch in range(cutn):\r\n",
        "    size = torch.randint(int(.5*sideX), int(.98*sideX), ())\r\n",
        "    offsetx = torch.randint(0, sideX - size, ())\r\n",
        "    offsety = torch.randint(0, sideX - size, ())\r\n",
        "    apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\r\n",
        "    apper = torch.nn.functional.interpolate(apper, (224,224), mode='bilinear')\r\n",
        "    p_s.append(nom(apper))\r\n",
        "  into = torch.cat(p_s, 0)\r\n",
        "\r\n",
        "  predict_clip = perceptor.encode_image(into)\r\n",
        "  factor = 100\r\n",
        "  loss = factor*(1-torch.cosine_similarity(predict_clip, target_clip, dim=-1).mean())\r\n",
        "  total_loss = loss\r\n",
        "  reg = torch.tensor(0., requires_grad=True)\r\n",
        "  if optimize_class and class_ent_reg:\r\n",
        "    reg = -factor*class_ent_reg*(class_vector_norm*torch.log(class_vector_norm+eps)).sum()\r\n",
        "    total_loss += reg\r\n",
        "  if i % save_every == 0:\r\n",
        "    checkin(total_loss.item(),loss.item(),reg.item(),class_vector_norm.detach().cpu().numpy(),out)\r\n",
        "  return total_loss\r\n",
        "\r\n",
        "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\r\n",
        "\r\n",
        "sample_num = 0\r\n",
        "for i in range(iterations):    \r\n",
        "  loss = ascend_txt(i)\r\n",
        "  optimizer.zero_grad()\r\n",
        "  loss.backward()\r\n",
        "  optimizer.step()\r\n",
        "\r\n",
        "out = '\"/content/%s.mp4\"'%prompt\r\n",
        "!ffmpeg -framerate $fps -i /content/output/frame_%05d.jpg -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart $out -y\r\n",
        "with open('/content/%s.mp4'%prompt, 'rb') as f:\r\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(f.read()).decode()\r\n",
        "clear_output()\r\n",
        "display(HTML(\"\"\"\r\n",
        "  <video controls autoplay loop>\r\n",
        "        <source src=\"%s\" type=\"video/mp4\">\r\n",
        "  </video>\"\"\" % data_url))\r\n",
        "\r\n",
        "from google.colab import files, output\r\n",
        "output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\r\n",
        "files.download('/content/%s.jpg'%prompt)\r\n",
        "files.download('/content/%s.mp4'%prompt)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}